---
title: "EcoTRACE Power Simulations"
author: "Emmanuel Guizar Rosales"
date: today
date-format: "[last rendered on:] MMM D, YYYY"
format:
  html:
    toc: true
    toc-depth: 5
    toc-expand: 2
    number-sections: true
    code-fold: true
    code-summary: "Show the code"
editor: visual
execute: 
  include: true
  echo: true
  message: false
  warning: false
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

```{r}
#| label: setup

# install package librarian if needed
if (!("librarian" %in% rownames(installed.packages()))) {
  install.packages("librarian")
}

# load required packages
librarian::shelf(
  tidyverse,
  tictoc,
  faux,
  lme4,
  afex,
  broom.mixed,
  sjPlot,
  ggpubr,
  ggthemes,
  DT,
  simr,
  latex2exp
)
```

::: callout-tip
## Executive Summary

-   For a range of potential parameters, a sample size of **400 subjects** and **25 items** should provide 95% power to detect a true smallest-effect size-of-interest (SEOI)-sized effect of *political orientation* on $\Delta Duration$.
-   Confidence in this estimate should be high enough to ask different research panel providers to offer initial estimates of costs.
-   The provided simulations and power analyses will probably not suffice for a well-grounded justification of sample size for a registered report (too many degrees of freedom for all parameters). Only a pilot study will provide justified assumptions for parameters (especially random effects).
-   Most important Figures: @fig-p.powC.effectsizes, @fig-p.powC.effectsizes.analytical, @fig-p.powC.error, @fig-p.powC.randomSlope, @fig-p.powC.items.
:::

# Introduction

In our mouselabWEB study, subjects repeatedly gather information regarding *carbon* or *bonus* outcomes of two choice options (*A* or *B*) by hovering over boxes hiding this information. We use a composite variable representing relative time spent gathering carbon and bonus information as primary dependent variable ($\Delta Duration$, with $t = DwellTime_{A} + DwellTime_{B}$)[^1]:

[^1]: In principle, we should calculate separate power analyses for all dependent variables we are planning to analyse in our study ($\Delta Duration$, $\Delta Frequency$, $First Cell$, $Last Cell$, and $Payne Index$). However, this is hardly feasible for two reasons. First, given the sparse literature on our topic, there seems to be little previous research that could inform our parameters, maybe with the exception of $\Delta Duration$. This is why the power analyses presented in this report focus on this dependent variable. Second, it would be too time consuming to do all power analyses for all the dependent variables.

$$
\Delta Duration = \frac{t_{Carbon} - t_{Bonus}}{t_{Carbon} + t_{Bonus}}
$$

The primary predictor of interest is subjects' *political orientation* (republican vs. democrat, between-subject factor). The individual choice situations (items) differ in their relative difference in carbon and bonus outcomes between options A and B.[^2] The key question is whether there is any difference in the relative time spent gathering carbon compared to bonus information between subjects identifying as democrats vs. republicans.

[^2]: Note that attributes of items are not of primary interest in this study.

In order to conduct an *a priori* power analysis without any pilot data, we need to simulate data.

# Simulation of $\Delta Duration$

Simulating $\Delta Duration$ requires us to think thoroughly about the data generating process that we suspect will be responsible for bringing about the values we are going to measure. In what follows, we reason that $\Delta Duration$ will be driven by subjects' political orientation by means of two processes. First, democrats will spend more time on looking on carbon information than republicans. Second, democrats will spend less time on looking on bonus information than republicans. Based on these opposing processes, we expect to find that democrats will display greater $\Delta Duration$ values than republicans. More specifically, democrats on average will be marked by positive $\Delta Duration$ values, whereas republicans will be marked by negative $\Delta Duration$ values.

We will model these two proposed processes in two seperate simulation functions. In a third function, we will combine data generated by these two functions in order to simulate the generation of $\Delta Duration$ values. These simulated $\Delta Duration$ values will then lay the groundwork for our power analyses.

In all our simulation functions, we will adhere to the following naming conventions to designate model parameters and sampled values:

-   `beta_*`: fixed effect parameters
-   `subj_*`: random effect parameters associated with subjects
-   `item_*`: random effect parameters associated with items
-   `X_*`: effect-coded predictor
-   `S_*`: sampled values for subject random effects
-   `I_*`: sampled values for item random effects

We will use letter abbreviations to designate fixed and random effects:

-   `*_0_*`: intercept
-   `*_p_*`: political orientation
-   `*_c`: carbon information
-   `*_b`: bonus information

Other terms:

-   `*_rho_*`: correlations; a vector of the upper right triangle for the correlation matrix for that group's random effects
-   `n_*`: sample size
-   `sigma`: residual (error) sd

## Simulation Functions

### Simulate Dwell Time on Carbon Information

We first need to simulate data representing how much time participants spend on gathering information regarding carbon outcomes. The choice for the default parameter values is justified as follows:

-   `n_subj`: **Number of subjects**. By default, we simulate data coming from **100** subjects. This seems to be a reasonable starting point for our simulations. Variations on sample size and its effects on power will be the topic of our power analyses.

-   `n_subj_prop`: **Proportion of republican and democrat subjects**. The first number represents the proportion of republicans and the second the number of democrats (Their sum needs to add up to 1). By default, we expect a **balanced proportion** (`c(0.50, 0.50)`) since we are aiming for a balanced sample. Variations on this assumption could be tested in our power analyses.

-   `n_item`: **Number of unique items in the Carbon Emission Task**. As in the original Carbon Emission Task [@Berger2021], we are planning on using a fully crossed design combining 5 levels of relative carbon outcome differences (10%, 15%, 20%, 50%, and 100%) and 5 levels of relative bonus outcome differences (same levels) between options A and B, that is **25** items. In principle, this number could be changed as well. In order to keep the structure of a fully crossed design, the number of trials should be the square of the number of levels of relative differences we would like to use (e.g., `r paste0((3:6)^2, sep = ", ", collapse = "")`...)

-   `beta_0_c`: **Fixed intercept (grand mean) for dwell time**. This value represents the number of seconds that subjects over all (irrespective of political orientation) spend on gathering carbon information in each choice situation (the effect of political orientation will be modeled as deviation from this grand mean). In a previous mouselabWEB study that follows a similar design (two options with two attributes) to study intertemporal choice [@reeck2017], study authors reported that subjects on average spent 7.3 seconds (*SD* = 1.1) acquiring information and making a decision. Based on this information, we define our defaults based on the expectation that subjects in total spend about 7 seconds for each choice. As a starting point, we simulate that subjects over all (irrespective of political orientation) spend about half of this time (**3.5** seconds) on gathering carbon outcome information.

-   `beta_p_c`: **Fixed effect of political orientation on dwell time**: This value represents the difference in average dwell times on carbon information between democrats and republicans (democrats - republicans). Coming up with a well-informed expectation of this effect is very difficult since no previous studies investigated the effect of political orientation on such information-seeking processes in a task with real environmental consequences. We therefore base our default value on a smallest effect size of interest informed by theoretical considerations [@lakens2022]. In mouselabWEB studies, it is a standard practice to filter out any information acquisition lasting less than about 200 msec because such very short (spurious) acquisitions are very unlikely to be consciously processed [@willemsen2019]. Thus, we define **0.2** seconds as a default difference in dwell time between democrats and republicans that could likely still reflect differences in conscious information gathering processes. Of course, we can study how varying this difference affects our power.

-   `subj_0_c`: **By-subject random intercept SD for dwell time**: We simulate that a subject's deviation from the grand mean for dwell time follows a normal distribution with a mean of 0 and a standard deviation of `subj_0_c`. Again, hypothesizing what this standard deviation will be, is very difficult. We base our default value on values reported in @reeck2017. They investigated whether variability in information search behavior is driven predominantly by differences in the features of a choice (e.g., in our case: the relative differences between carbon and bonus outcomes in options A and B) or by individual differences. To this end, they predicted information acquisition using a intercept-only model that included random intercepts for subjects and items.[^3] They estimated the random intercept of subjects to be 0.29 (95%-CI = 0.26-0.32). The estimated random intercept for items was 0.04 (95%-CI = 0.02-0.06). That is, the subject random effect was about 6 times higher than the item random effect, demonstrating that individual differences played a more important role in determining search behavior than the features of the options being presented. Therefore, we set our default value for the by-subject random intercept SD (**0.25**, i.e. 7.25% of the fixed intercept)[^4] to be about six times the size of the by-item random intercept (0.04, see below).

-   `subj_p_c`: **By-subject random slope SD for dwell time**: Most probably, the effect of political orientation on carbon information dwell time will differ between participants. The distribution of this random slope is difficult to guess a priori given the sparse previous literature that could inform our study. As a (conservative) default, we model the SD of the random slope to be as big as the fixed effect itself, i.e. **0.20**. That is, we expect that for about 2/3 of subjects, the effect of political orientation lies between 0.2 - 0.2 = 0 and 0.2 + 0.2 = 0.4.

-   `subj_rho_c`: **By-subject random effects correlation**: Since we model that not only the fixed intercept but also the fixed effect of political orientation can vary between subjects, it is possible that subjects' random intercepts and slopes are correlated. For simplicity's sake, however, we will assume no correlation (**0.00**). Note, however, that such correlations could easily be modeled as well -- and there might be good reason to do so.[^5]

-   `item_0_c`: **By-item random intercept SD for dwell time**: Based on considerations explained above, we set this value to `(1/6)*subj_0_c` = **0.04**.

-   `sigma_c`: **Trial-level noise (error) SD**: Following suggestions by @debruine2021, we model this error standard deviation to be twice the size of the by-subject variance components, i.e., to `2*(0.25 + 0.2)` = **`r 2*(0.25 + 0.2)`**.

[^3]: The variable they predicted was the number of transitions between options made by each subject on each trial for each of six possible transitions. Although this is a different dependent variable than dwell time, it is nevertheless a process-tracing measure which might allow to draw some conclusions for our goals. The average number of transitions for each trial was 4.0 (SD = 1.2).

[^4]: @reeck2017 report a by-subject random intercept of 0.29 and an average number of transitions of 4.0, that is the ratio of random intercept SD to fixed intercept is `.29/4` = `r .29/4`.

[^5]: For instance, it is conceivable that subjects who have a greater than average total dwell time on carbon information also have a greater effect of political orientation on these dwell times. We could capture this by allowing for a small positive correlation between these two random effects of, e.g., **0.10**.

```{r}
#| label: prep-fun-sim-dtCarbon

# set up the custom data simulation function for dwell time carbon
FUN_sim_dwellTimeCarbon <- function(
  n_subj       =       100, # number of subjects
  n_subj_prop  = c(.5, .5), # proportion of republican and democrat subjects
  n_item       =        25, # number of items
  beta_0_c     =       3.5, # intercept (grand mean) for dwell time (dt) carbon
  beta_p_c     =       .20, # effect of political orientation on dt carbon
  subj_0_c     =       .25, # by-subject random intercept sd for dt carbon
  subj_p_c     =       .20, # by-subject random slope sd for dt carbon
  subj_rho_c   =       .00, # by-subject random effects correlations for dt carbon
  item_0_c     =       .04, # by-item random intercept sd
  sigma_c      =        .9, # residual (error) sd
  
  options = list(
    truncNegNums = FALSE    # set negative numbers to zero (no negative dwell times)
  )
) {
  
  # simulate data for dwell time on carbon information
  dat_c <- 
    # add random factor subject
    add_random(subj = n_subj) %>% 
    # add random factor item
    add_random(item = n_item) %>% 
    # add between-subject factor political orientation (with anova contrast)
    # note that it is important that we set .shuffle = FALSE. This way, the first n_subj_prop
    # will be assigned polOri = rep and the rest will be assigned polOri = dem.
    # if we do not follow this procedure, then it can and will happen that in the
    # FUN_sim_dwellTimeBonus the same subject is assigned a different political orientation!
    add_between("subj", polOri = c("rep", "dem"), .prob = n_subj_prop*n_subj, .shuffle = FALSE) %>% 
    add_contrast("polOri", colnames = "X_p", contrast = "anova") %>% 
    # add by-subject random intercept and slope
    add_ranef("subj", S_0_c = subj_0_c, S_p_c = subj_p_c, .cors = subj_rho_c) %>% 
    # add by-item random intercept
    add_ranef("item", I_0_c = item_0_c) %>% 
    # add error term
    add_ranef(e_si_c = sigma_c) %>% 
    # add response values
    mutate(
      # add together fixed and random effects for each effect
      B_0 = beta_0_c + S_0_c + I_0_c,
      B_p = beta_p_c + S_p_c,
      # calculate dv by adding each effect term multiplied by the relevant
      # effect-coded factors
      dv_c = B_0 + (B_p * X_p) + e_si_c
    )
  
  # truncuate negative numbers?
  if (options$truncNegNums) {
    dat_c <- dat_c %>%
      mutate(dv_c = ifelse(dv_c < 0, 0, dv_c))
  }
  
  # run a linear mixed effects model and check summary
  mod_c <- lmer(
    dv_c ~ polOri + (1 + polOri | subj) + (1 | item),
    data = dat_c
  )
  mod_c.sum <- summary(mod_c)
  
  # check groups
  tab_groups <- mod_c.sum$ngrps %>%
    as_tibble(rownames = "Random.Factor") %>%
    mutate(parameters = c(n_subj, n_item))

  # check random effects
  tab_ranef <- mod_c.sum$varcor %>%
    as_tibble() %>%
    select(Groups = grp, Name1 = var1, Name2 = var2, "Std.Dev." = sdcor) %>%
    mutate(parameters = c(subj_0_c, subj_p_c, subj_rho_c, item_0_c, sigma_c))
  
  # check fixed effects
  tab_fixef <- mod_c.sum$coefficients %>% 
    as_tibble(rownames = "Effect") %>% 
    select(Effect, Estimate) %>% 
    mutate(parameters = c(beta_0_c, beta_p_c))

  # combine these checks in one list
  list_checks <- list(
    groups = tab_groups,
    random_effects = tab_ranef,
    fixed_effects = tab_fixef
  )
  
  # create plot to check simulation
  plot_dat_c <- dat_c %>% 
    ggplot(aes(x = polOri, y = dv_c, color = polOri)) +
    geom_hline(yintercept = beta_0_c) +
    geom_violin(alpha = .5) +
    stat_summary(
      fun = mean,
      fun.min = \(x){mean(x) - sd(x)},
      fun.max = \(x){mean(x) + sd(x)}
    ) +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_color_manual(values = c("red", "dodgerblue")) +
    theme_bw()
    
    
    
    return(list(
      data = dat_c,
      model_lmer = mod_c,
      checks = list_checks,
      plot = plot_dat_c
    ))
}
```

Let's have a look at the simulated data. First, let's have a look at @fig-sim-dtCarbon, displaying the distribution (violin plot) and mean ± 1 standard deviation (dot ± error bars) for republicans and democrats. The true population mean is indicated by the horizontal line.

```{r}
#| label: fig-sim-dtCarbon
#| fig-cap: |
#|   Distribution of dwell times on carbon outcomes (*dv_c*).

out_fun_sim_dwellTimeCarbon <- FUN_sim_dwellTimeCarbon(
  n_subj       =       100, # number of subjects
  n_subj_prop  = c(.5, .5), # number of republican and democrat subjects
  n_item       =        25, # number of items
  beta_0_c     =       3.5, # intercept (grand mean) for dwell time (dt) carbon
  beta_p_c     =       .20, # effect of political orientation on dt carbon
  subj_0_c     =       .25, # by-subject random intercept sd for dt carbon
  subj_p_c     =       .20, # by-subject random slope sd for dt carbon
  subj_rho_c   =       .00, # by-subject random effects correlations for dt carbon
  item_0_c     =       .04, # by-item random intercept sd
  sigma_c      =        .9, # residual (error) sd
  
  options = list(
    truncNegNums = FALSE    # set negative numbers to zero (no negative dwell times)
  )
)

out_fun_sim_dwellTimeCarbon$plot
```

Next, we compare the estimated values and population (i.e., "true") parameters:

```{r}
#| label: tbl-sim-dtCarbon-groups
out_fun_sim_dwellTimeCarbon$checks$groups %>% 
  knitr::kable()
```

```{r}
#| label: tbl-sim-dtCarbon-ranef
out_fun_sim_dwellTimeCarbon$checks$random_effects %>% 
  knitr::kable()
```

```{r}
#| label: tbl-sim-dtCarbon-fixef
out_fun_sim_dwellTimeCarbon$checks$fixed_effects %>% 
  knitr::kable()
```

### Simulate Dwell Time on Bonus Information

Next, we need to simulate data representing how much time participants spend on gathering information regarding bonus outcomes. The choice for the default parameter values is justified as above. Most important difference: `beta_p_b` is negative as we expect that compared to republicans, democrats spend **less** time on gathering bonus information. Specifically, we reason that republicans spend the time they "save" in looking at carbon information (reduced dwell time on carbon information compared to democrats) on looking at bonus information, i.e., `beta_p_b = -beta_p_c = -0.20`.

```{r}
#| label: prep-fun-sim-dtBonus

# set up the custom data simulation function for dwell time bonus
FUN_sim_dwellTimeBonus <- function(
  n_subj       =       100, # number of subjects
  n_subj_prop  = c(.5, .5), # number of republican and democrat subjects
  n_item       =        25, # number of items
  beta_0_b     =       3.5, # intercept (grand mean) for dwell time (dt) bonus
  beta_p_b     =      -.20, # effect of political orientation on dt bonus
  subj_0_b     =       .25, # by-subject random intercept sd for dt bonus
  subj_p_b     =       .20, # by-subject random slope sd for dt bonus
  subj_rho_b   =       .00, # by-subject random effects correlations for dt bonus
  item_0_b     =       .04, # by-item random intercept sd for dwell time bonus
  sigma_b      =        .9, # residual (error) sd for dwell time bonus
  
  options = list(
    truncNegNums = FALSE    # set negative numbers to zero (no negative dwell times)
  )
) {
  
  # simulate data for dwell time on carbon information
  dat_b <- 
    # add random factor subject
    add_random(subj = n_subj) %>% 
    # add random factor item
    add_random(item = n_item) %>% 
    # add between-subject factor political orientation (with anova contrast)
    # note: see comment on .shuffle = FALSE in FUN_sim_dwellTimeCarbon
    add_between("subj", polOri = c("rep", "dem"), .prob = n_subj_prop*n_subj, .shuffle = FALSE) %>% 
    add_contrast("polOri", colnames = "X_p", contrast = "anova") %>% 
    # add by-subject random intercept and slope
    add_ranef("subj", S_0_b = subj_0_b, S_p_b = subj_p_b, .cors = subj_rho_b) %>% 
    # add by-item random intercept
    add_ranef("item", I_0_b = item_0_b) %>% 
    # add error term
    add_ranef(e_si_b = sigma_b) %>% 
    # add response values
    mutate(
      # add together fixed and random effects for each effect
      B_0 = beta_0_b + S_0_b + I_0_b,
      B_p = beta_p_b + S_p_b,
      # calculate dv by adding each effect term multiplied by the relevant
      # effect-coded factors
      dv_b = B_0 + (B_p * X_p) + e_si_b
    )
  
  # truncuate negative numbers?
  if (options$truncNegNums) {
    dat_b <- dat_b %>%
      mutate(dv_b = ifelse(dv_b < 0, 0, dv_b))
  }
  
  # run a linear mixed effects model and check summary
  mod_b <- lmer(
    dv_b ~ polOri + (1 + polOri | subj) + (1 | item),
    data = dat_b
  )
  mod_b.sum <- summary(mod_b)
  
  # check groups
  tab_groups <- mod_b.sum$ngrps %>%
    as_tibble(rownames = "Random.Factor") %>%
    mutate(parameters = c(n_subj, n_item))

  # check random effects
  tab_ranef <- mod_b.sum$varcor %>%
    as_tibble() %>%
    select(Groups = grp, Name1 = var1, Name2 = var2, "Std.Dev." = sdcor) %>%
    mutate(parameters = c(subj_0_b, subj_p_b, subj_rho_b, item_0_b, sigma_b))
  
  # check fixed effects
  tab_fixef <- mod_b.sum$coefficients %>% 
    as_tibble(rownames = "Effect") %>% 
    select(Effect, Estimate) %>% 
    mutate(parameters = c(beta_0_b, beta_p_b))

  # combine these checks in one list
  list_checks <- list(
    groups = tab_groups,
    random_effects = tab_ranef,
    fixed_effects = tab_fixef
  )
  
  # create plot to check simulation
  plot_dat_b <- dat_b %>% 
    ggplot(aes(x = polOri, y = dv_b, color = polOri)) +
    geom_hline(yintercept = beta_0_b) +
    geom_violin(alpha = .5) +
    stat_summary(
      fun = mean,
      fun.min = \(x){mean(x) - sd(x)},
      fun.max = \(x){mean(x) + sd(x)}
    ) +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_color_manual(values = c("red", "dodgerblue")) +
    theme_bw()
    
    
    
    return(list(
      data = dat_b,
      model_lmer = mod_b,
      checks = list_checks,
      plot = plot_dat_b
    ))
}
```

Again, let's have a look at the simulated data. First, let's have a look at @fig-sim-dtBonus, displaying the distribution (violin plot) and mean ± 1 standard deviation (dot ± error bars) for republicans and democrats. The true population mean is indicated by the horizontal line.

```{r}
#| label: fig-sim-dtBonus
#| fig-cap: |
#|   Distribution of dwell times on bonus outcomes.

out_fun_sim_dwellTimeBonus <- FUN_sim_dwellTimeBonus(
  n_subj       =       100, # number of subjects
  n_subj_prop  = c(.5, .5), # number of republican and democrat subjects
  n_item       =        25, # number of items
  beta_0_b     =       3.5, # intercept (grand mean) for dwell time (dt) bonus
  beta_p_b     =      -.20, # effect of political orientation on dt bonus
  subj_0_b     =       .25, # by-subject random intercept sd for dt bonus
  subj_p_b     =       .20, # by-subject random slope sd for dt bonus
  subj_rho_b   =       .00, # by-subject random effects correlations for dt bonus
  item_0_b     =       .04, # by-item random intercept sd for dwell time bonus
  sigma_b      =        .9, # residual (error) sd for dwell time bonus
  
  options = list(
    truncNegNums = FALSE    # set negative numbers to zero (no negative dwell times)
  )
)

out_fun_sim_dwellTimeBonus$plot
```

Next, we compare the estimated values and population (i.e., "true") parameters:

```{r}
#| label: tbl-sim-dtBonus-groups
out_fun_sim_dwellTimeBonus$checks$groups %>% 
  knitr::kable()
```

```{r}
#| label: tbl-sim-dtBonus-ranef
out_fun_sim_dwellTimeBonus$checks$random_effects %>% 
  knitr::kable()
```

```{r}
#| label: tbl-sim-dtBonus-fixef
out_fun_sim_dwellTimeBonus$checks$fixed_effects %>% 
  knitr::kable()
```

Finally, we inspect the `lmer` model results that modelled the data exactly as the population data was set up:

`dv ~ polOri + (1 + polOri | subj) + (1 | item)`

Note that in the following table random effects represent variances (not standard deviations), except for the error term.

```{r}
#| label: res-sim-dwellTimeCarbonAndBonus
tab_model(out_fun_sim_dwellTimeCarbon$model_lmer, out_fun_sim_dwellTimeBonus$model_lmer,
          show.ci = FALSE, digits = 4, digits.re = 4)
```

### Simulate $\Delta Duration$

We are now ready to combine the two functions above to simulate $\Delta Duration$ data.

```{r}
#| label: prep-fun-sim-deltaDuration

# set up the custom data simulation function for delta duration
FUN_sim_dwellTimeDeltaDuration <- function(
  n_subj       =       100, # number of subjects
  n_subj_prop  = c(.5, .5), # number of republican and democrat subjects
  n_item       =        25, # number of items
  beta_0_c     =       3.5, # intercept (grand mean) for dwell time (dt) carbon
  beta_p_c     =       .20, # effect of political orientation on dt carbon
  beta_0_b     =       3.5, # intercept (grand mean) for dwell time (dt) bonus
  beta_p_b     =      -.20, # effect of political orientation on dt bonus
  subj_0_c     =       .25, # by-subject random intercept sd for dt carbon
  subj_p_c     =       .20, # by-subject random slope sd for dt carbon
  subj_rho_c   =       .00, # by-subject random effects correlations for dt carbon
  subj_0_b     =       .25, # by-subject random intercept sd for dt bonus
  subj_p_b     =       .20, # by-subject random slope sd for dt bonus
  subj_rho_b   =       .00, # by-subject random effects correlations for dt bonus
  item_0_c     =       .04, # by-item random intercept sd for carbon
  item_0_b     =       .04, # by-item random intercept sd for bonus
  sigma_c      =        .9, # residual (error) sd for carbon
  sigma_b      =        .9, # residual (error) sd for bonus
  
  options = list(
    truncNegNums = TRUE     # set negative numbers to zero (no negative dwell times)
  )
) {
  
  # simulate data for dwell time carbon
  out_fun_sim_dwellTimeCarbon <- FUN_sim_dwellTimeCarbon(
    n = n_subj,
    n_subj_prop = n_subj_prop,
    n_item = n_item,
    beta_0_c = beta_0_c,
    beta_p_c = beta_p_c,
    subj_0_c = subj_0_c,
    subj_p_c = subj_p_c,
    subj_rho_c = subj_rho_c,
    item_0_c = item_0_c,
    sigma_c = sigma_c,
    options = list(truncNegNums = TRUE)
  )
  
  # simulate data for dwell time bonus
  out_fun_sim_dwellTimeBonus <- FUN_sim_dwellTimeBonus(
    n = n_subj,
    n_subj_prop = n_subj_prop,
    n_item = n_item,
    beta_0_b = beta_0_b,
    beta_p_b = beta_p_b,
    subj_0_b = subj_0_b,
    subj_p_b = subj_p_b,
    subj_rho_b = subj_rho_b,
    item_0_b = item_0_b,
    sigma_b = sigma_b,
    options = list(truncNegNums = TRUE)
  )
  
  # join data sets and create variable delta duration
  dat <- left_join(
    x = out_fun_sim_dwellTimeCarbon$data %>% select(-c(B_0, B_p)),
    y = out_fun_sim_dwellTimeBonus$data %>% select(-c(B_0, B_p)),
    by = c("subj", "item", "polOri", "X_p")
  ) %>% 
    # create index delta duration
    mutate(dv_index = (dv_c - dv_b) / (dv_c + dv_b)) %>% 
    # reorder variables so that dv_index is more at the beginning
    select(1:4, dv_index, dv_c, dv_b, everything())
  
  # run a linear mixed effects model and check summary
  mod <- lmer(
    dv_index ~ polOri + (1 + polOri | subj) + (1 | item),
    data = dat
  )
  mod.sum <- summary(mod)
  
  # check groups
  tab_groups <- mod.sum$ngrps %>%
    as_tibble(rownames = "Random.Factor") %>% 
    mutate(parameters = c(n_subj, n_item))

  # check random effects
  tab_ranef <- mod.sum$varcor %>%
    as_tibble() %>%
    select(Groups = grp, Name1 = var1, Name2 = var2, "Std.Dev." = sdcor)
  
  # check fixed effects
  tab_fixef <- mod.sum$coefficients %>% 
    as_tibble(rownames = "Effect") %>% 
    select(Effect, Estimate) %>% 
    mutate(parameters = c(
      NA,
      ( (beta_0_c + beta_p_c) - (beta_0_b + beta_p_b) ) / (beta_0_c + beta_p_c + beta_0_b + beta_p_b)
    ))

  # combine these checks in one list
  list_checks <- list(
    groups = tab_groups,
    random_effects = tab_ranef,
    fixed_effects = tab_fixef
  )
  
  # create plots
  plot_dvs <- dat %>% 
    pivot_longer(cols = c("dv_c", "dv_b"), names_to = "dv") %>%
    mutate(dv = factor(dv, levels = c("dv_c", "dv_b"))) %>% 
    ggplot(aes(x = polOri, y = value, color = polOri)) +
    geom_violin(alpha = .5, show.legend = FALSE) +
    stat_summary(
      fun = mean,
      fun.min = \(x){mean(x) - sd(x)},
      fun.max = \(x){mean(x) + sd(x)},
      show.legend = FALSE
    ) +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_color_manual(values = c("red", "dodgerblue")) +
    theme_bw() +
    facet_wrap(~ dv)
  
  plot_dv_index <- dat %>% 
    ggplot(aes(x = polOri, y = dv_index, color = polOri)) +
    geom_violin(alpha = .5, show.legend = FALSE) +
    stat_summary(
      fun = mean,
      fun.min = \(x){mean(x) - sd(x)},
      fun.max = \(x){mean(x) + sd(x)},
      show.legend = FALSE
    ) +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    coord_cartesian(ylim = c(-1, 1)) +
    scale_color_manual(values = c("red", "dodgerblue")) +
    theme_bw()
  
  return(list(
    data = dat,
    model_lmer = mod,
    checks = list_checks,
    plots = list(dvs = plot_dvs,
                 dv_index = plot_dv_index),
    indFunOut = list(carbon = out_fun_sim_dwellTimeCarbon,
                     bonus = out_fun_sim_dwellTimeBonus)
  ))
  
}
```

We check the result of one particular simulation as an example by inspecting @fig-sim-check-deltaDuration.

```{r}
#| label: prep-callFun-deltaDuration

out_fun_sim_dwellTimeDeltaDuration <- FUN_sim_dwellTimeDeltaDuration(
  n_subj       =       100, # number of subjects
  n_subj_prop  = c(.5, .5), # number of republican and democrat subjects
  n_item       =        25, # number of items
  beta_0_c     =       3.5, # intercept (grand mean) for dwell time (dt) carbon
  beta_p_c     =       .20, # effect of political orientation on dt carbon
  beta_0_b     =       3.5, # intercept (grand mean) for dwell time (dt) bonus
  beta_p_b     =      -.20, # effect of political orientation on dt bonus
  subj_0_c     =       .25, # by-subject random intercept sd for dt carbon
  subj_p_c     =       .20, # by-subject random slope sd for dt carbon
  subj_rho_c   =       .00, # by-subject random effects correlations for dt carbon
  subj_0_b     =       .25, # by-subject random intercept sd for dt bonus
  subj_p_b     =       .20, # by-subject random slope sd for dt bonus
  subj_rho_b   =       .00, # by-subject random effects correlations for dt bonus
  item_0_c     =       .04, # by-item random intercept sd for carbon
  item_0_b     =       .04, # by-item random intercept sd for bonus
  sigma_c      =        .9, # residual (error) sd for carbon
  sigma_b      =        .9, # residual (error) sd for bonus
  
  options = list(
    truncNegNums = TRUE     # set negative numbers to zero (no negative dwell times)
  )
)
```

```{r}
#| label: fig-sim-check-deltaDuration
#| fig-cap: |
#|   Results of one simulation of $\Delta Duration$ data. **(A)** simulated
#|   dwell times on carbon information (*dv_c*) and on bonus information
#|   (*dv_b*). **(B)** Resulting distributions of $\Delta Duration$ indices.
ggarrange(
  out_fun_sim_dwellTimeDeltaDuration$plots$dvs +
    labs(y = "Dwell Time (in s)"),
  out_fun_sim_dwellTimeDeltaDuration$plots$dv_index +
    labs(y = TeX(r'($\Delta Duration$)')),
  labels = c("A", "B"),
  ncol = 2, widths = c(2, 1)
)
```

We predict $\Delta Duration$ by the following formula:

`dv_index ~ polOri + (1 + polOri | subj) + (1 | item)`

The results of the `lmer` model are:

```{r}
#| label: tab-sim-check-deltaDuration
tab_model(out_fun_sim_dwellTimeDeltaDuration$model_lmer,
          show.ci = FALSE, digits = 4, digits.re = 4)
```

## Create Multiple Simulations

In a next step, we repeat the simulation 1000 times to get more robust estimates of fixed and random effects we should expect based on the parameters we provided.

```{r}
#| label: prep-multiSim-deltaDuration

# how many times should the simulations be run?
reps <- 1000

# define arguemnts
args <- list(
  n_subj       =       100, # number of subjects
  n_subj_prop  = c(.5, .5), # number of republican and democrat subjects
  n_item       =        25, # number of items
  beta_0_c     =       3.5, # intercept (grand mean) for dwell time (dt) carbon
  beta_p_c     =       .20, # effect of political orientation on dt carbon
  beta_0_b     =       3.5, # intercept (grand mean) for dwell time (dt) bonus
  beta_p_b     =      -.20, # effect of political orientation on dt bonus
  subj_0_c     =       .25, # by-subject random intercept sd for dt carbon
  subj_p_c     =       .20, # by-subject random slope sd for dt carbon
  subj_rho_c   =       .00, # by-subject random effects correlations for dt carbon
  subj_0_b     =       .25, # by-subject random intercept sd for dt bonus
  subj_p_b     =       .20, # by-subject random slope sd for dt bonus
  subj_rho_b   =       .00, # by-subject random effects correlations for dt bonus
  item_0_c     =       .04, # by-item random intercept sd for carbon
  item_0_b     =       .04, # by-item random intercept sd for bonus
  sigma_c      =        .9, # residual (error) sd for carbon
  sigma_b      =        .9, # residual (error) sd for bonus
  
  options = list(
    truncNegNums = TRUE     # set negative numbers to zero (no negative dwell times)
  )
)

# define function to extract estimated fixed and random effects
getFixedAndRandEff_sim_dwellTimeDeltaDuration <- function(rep) {
  
  # do the simulation once
  sim <- do.call(FUN_sim_dwellTimeDeltaDuration, args)
  
  # put the fixed effects into a data table
  fixed_effects <- broom.mixed::tidy(sim$model_lmer, "fixed") %>% 
    # add a column for each repetition
    mutate(rep = rep)
  
  # put the random effects into a data table
  random_effects <- broom.mixed::tidy(sim$model_lmer, "ran_pars") %>% 
    # add a column for each repetition
    mutate(rep = rep)
  
  return(list(
    fix = fixed_effects,
    rand = random_effects
  ))
}
```

```{r}
#| label: prep-multiSim-deltaDuration-doSims
#| eval: false

# do the repetitions and save results in a list
# time this function with tictoc
tic()
repetitions <- map(
  .x = 1:reps,
  .f = getFixedAndRandEff_sim_dwellTimeDeltaDuration
)
toc()

# fixed effects: combine results into one data frame
fixed_multiSim_dwellTimeDeltaDuration <- map_dfr(
  .x = 1:reps,
  .f = ~repetitions[[.x]]$fix
)

# random effects: combine results into one data frame
random_multiSim_dwellTimeDeltaDuration <- map_dfr(
  .x = 1:reps,
  .f = ~repetitions[[.x]]$rand
) %>% 
  mutate(group_term = str_c(group, term, sep = "_"))

# save simulation results in a data frame
time <- format(Sys.time(), "%Y%m%d_%H%M")
fileName <- paste0("SimCustom_defaultValues", "_", time, ".RData")

save(
  fixed_multiSim_dwellTimeDeltaDuration,
  random_multiSim_dwellTimeDeltaDuration,
  file = file.path("../data/powerAnalyses", fileName)
)
```

```{r}
#| label: prep-multiSim-deltaDuration-loadSims

# load stored simulations (change file name if necessary)
fileName <- "SimCustom_defaultValues_20240304_0823.RData"
load(file.path("../data/powerAnalyses", fileName))
```

Let's have a look at the distribution of fixed effects in these multiple simulations. These are shown in @fig-multiSim-deltaDuration-fixedEff.[^6]

[^6]: Note that the fixed effect of `polOri` should be close to: `r out_fun_sim_dwellTimeDeltaDuration$checks$fixed_effects$parameters[2]`. For explanations, see @sec-analyticalApproach.

```{r}
#| label: fig-multiSim-deltaDuration-fixedEff
#| fig-cap: |
#|   Distribution of estimates of fixed effects. Dashed lines indicate
#|   distribution means.

# create data set of means of distributions
means_fixed <- fixed_multiSim_dwellTimeDeltaDuration %>% 
  group_by(term) %>% 
  summarise(estimate.mean = round(mean(estimate), 4))

# plot distributions
fixed_multiSim_dwellTimeDeltaDuration %>% 
  ggplot(aes(x = estimate)) +
  geom_histogram(
    color = "black",
    fill = "grey80",
    alpha = .5,
    show.legend = FALSE
  ) +
  geom_vline(
    data = means_fixed,
    mapping = aes(xintercept = estimate.mean),
    linetype = "dashed",
    linewidth = 1
  ) +
  geom_label(
    data = means_fixed,
    mapping = aes(x = estimate.mean, label = estimate.mean),
    y = Inf,
    vjust = 1.5
  ) +
  # geom_label(aes(x = meanEff, y = 100, label = meanEff)) +
  facet_wrap(~term, scales = "fixed") +
  theme_bw()
```

Let's calculate the power of our analysis to find these effects.

```{r}
#| label: tbl-power-defaultValues
# calculate power
fixed_multiSim_dwellTimeDeltaDuration %>% 
  group_by(term) %>% 
  summarise(
    power = mean(p.value < 0.05)
  ) %>% 
  knitr::kable(digits = 3)
```

Finally, let's have a look at the distribution of random effects in these multiple simulations. These are shown in @fig-multiSim-deltaDuration-randEff.

```{r}
#| label: fig-multiSim-deltaDuration-randEff
#| fig-cap: |
#|   Distribution of estimates of random effects. Dashed lines indicate
#|   distribution means.

# create data set of means of distributions
means_random <- random_multiSim_dwellTimeDeltaDuration %>% 
  group_by(group_term) %>% 
  summarise(estimate.mean = round(mean(estimate, na.rm = TRUE), 4))

# plot distributions
random_multiSim_dwellTimeDeltaDuration %>% 
  ggplot(aes(x = estimate)) +
  geom_histogram(
    color = "black",
    fill = "grey80",
    alpha = .5,
    show.legend = FALSE
  ) +
  geom_vline(
    data = means_random,
    mapping = aes(xintercept = estimate.mean),
    linetype = "dashed",
    linewidth = 1
  ) +
  geom_label(
    data = means_random,
    mapping = aes(x = estimate.mean, label = estimate.mean),
    y = Inf,
    vjust = 1.5
  ) +
  facet_wrap(~group_term, scales = "free") +
  theme_bw()
```

These estimated effects, especially the random ones, can now be helpful in order to do power simulations using the [`simr`](https://cran.r-project.org/web/packages/simr/index.html) package.

# Power Simulations

## Simulations with `simr`

### Define Parameters for `simr`

Here, we leverage the package `simr` to do multiple power simulations. We need to define a design table data frame and the expected fixed and random effects parameters. We do not need to simulate the dependent variable. This will be done by `simr::makeLmer` later on.

```{r}
#| label: tbl-designTableForSimr-parameters
#| tbl-cap: |
#|   Parameters for design data table for `simr` functions.

# define number of subjects
n_subject <- 100
# define number of trials
n_item <- 25

# create basic data frame
dataSimr <- add_random(subj = n_subject, item = n_item) %>% 
  # this step is needed for simr to work properly
  mutate(subj = factor(subj),
         item = factor(item)) %>% 
  # add between-subject factor political orientation (republican vs. democrat)
  add_between("subj", polOri = c("rep", "dem")) %>% 
  # add anova style contrasts (intercept = grand mean)
  add_contrast("polOri", contrast = "anova")

# create temporary tibble to get number of republicans and democrats
tmp <- dataSimr %>% 
    distinct(subj, polOri) %>% 
    count(polOri)

# create tibble to display design variables
tibble(
  parameter = c("Number of Subjects",
                "Number of Items",
                "Number of Republicans",
                "Number of Democrats"),
  value = c(
    length(unique(out_fun_sim_dwellTimeDeltaDuration$data$subj)),
    length(unique(out_fun_sim_dwellTimeDeltaDuration$data$item)),
    tmp$n[1],
    tmp$n[2]
  )
) %>% 
  knitr::kable()
```

```{r}
#| label: tbl-designTableForSimr
#| tbl-cap: |
#|   Design data table for `simr` functions.
dataSimr %>% 
  DT::datatable()
```

As before, we will model $\Delta Duration$ as follows:

$\Delta Duration \sim polOri + (1 + polOri | subj) + (1 | item)$

For the fixed and random effects, we will use the mean of the estimated effects in our previous simulations (see @fig-multiSim-deltaDuration-fixedEff and @fig-multiSim-deltaDuration-randEff).[^7]

[^7]: Note, however, that we cannot include all random effects exactly as displayed in @fig-multiSim-deltaDuration-randEff. Each pair of by-subject random intercept is associated with a specific by-subject random slope based on their correlation. That is, we cannot simply provide all these three parameters separately. To deal with this issue, we will first work with models that assume zero correlation between by-subject random intercepts and slopes.

```{r}
#| label: prep-understandingSimrVarCor
#| eval: false

# see https://github.com/pitakakariki/simr/issues/132
# 
model <- lmer(dv_index ~ polOri + (1 + polOri | subj) + (1 | item),
              data = out_fun_sim_dwellTimeDeltaDuration$data)

# We need the following values for the following matrix:
# 
# matrix(A,B,
#        C,D)
# 
# A = by-subject intercept variance. We can either get this by squaring the 
#     Std.Dev = 0.037993 or by reading it of if we loaded lmerTest
# B = covariance of by-subject intercept and by-subject slope. We get this by
#     multiplying the by-subject intercept Std.Dev, by-subject slope Std.Dev,
#     and Corr.
# C = by-subject slope variance. We can either get this by squaring the
#     Std.Dev = 0.009537 or by reading it of if we loaded lmerTest.
# D = B

# first, we inspect the results of our model to identify these values:
summary(model)

# then we define our VarCor matrix as described
myMatrix <- matrix(c(
  0.037993^2,               0.037993*0.009537*0.85,
  0.037993*0.009537*0.85,   0.009537^2
), 2)

# then we define the list as we need it for simr
# note that as the second element, we provide Std.Dev squared (i.e., variance)
# for the by-item intercept (which has no random slope)
V <- list(
  myMatrix,
  0
)

# finally, we call makeLmer with our values defined and check weather the
# resulting model summary is as expected.
makeLmer(
  dv_index ~ polOri + (1 + polOri | subj) + (1 | item),
  fixef=c(-0.0293, 0.0592),
  VarCorr = V,
  sigma = 0.188492,
  data = out_fun_sim_dwellTimeDeltaDuration$data
)
```

```{r}
#| label: prep-makeLmer-parameters

# define fixed effects
fixef <- means_fixed$estimate.mean

# extract and calculate variables for VarCorr[[1]] covariance matrix. e designates
# elements of the matrix:
# matrix(e1, e2
#        e3, e4)

# e1: by-subject intercept variance
e1 <- means_random %>% 
  filter(group_term == "subj_sd__(Intercept)") %>% 
  pull(estimate.mean) %>% 
  # square sd to get variance
  .^2

# e4: by-subject slope variance
e4 <- means_random %>% 
  filter(group_term == "subj_sd__polOridem") %>% 
  pull(estimate.mean) %>% 
  # square sd to get variance
  .^2

# e2=e3: covariance of by-subject intercept and by-subject slope
tmp.corr <- means_random %>% 
  filter(group_term == "subj_cor__(Intercept).polOridem") %>% 
  pull(estimate.mean)
# covariance = corr * sd * sd
e2 <- tmp.corr*sqrt(e1)*sqrt(e4)
e3 <- e2

# create matrix for VarCorr[[1]]
V <- matrix(c(
  e1, e2,
  e3, e4
), nrow = 2)

# extract by-item intercept variance
e5 <- means_random %>% 
  filter(group_term == "item_sd__(Intercept)") %>% 
  pull(estimate.mean) %>% 
  .^2

# define VarCorr
VarCorr <- list(
  V,
  e5
)

# extract residuals (need to be provided as sd!)
sigma <- means_random %>% 
  filter(group_term == "Residual_sd__Observation") %>% 
  pull(estimate.mean)
```

Applying `simr::makeLmer`, we generated the following model (compare output with values reported in @fig-multiSim-deltaDuration-fixedEff and @fig-multiSim-deltaDuration-randEff).

```{r}
#| label: prep-makeLmer-callMakeLmer
s0.makeLmer <- makeLmer(
  y ~ polOri + (1 + polOri | subj) + (1 | item),
  fixef=fixef,
  VarCorr = VarCorr,
  sigma = sigma,
  data = dataSimr
)

# inspect results
# s0.makeLmer

# note that our variance-covariance matrix resulted in slightly different results.
# for this reaseon, we change the basic model to not include a correaltion between
# by-subject random intercept and slope
V_new <- matrix(c(
  e1, 0,
  0, e4), nrow = 2)
VarCorr(s0.makeLmer) <- list(
  V_new,
  e5
)

# inspect the model generated by makeLmer again
s0.makeLmer
```

The power of this model as calculated by `simr` using 1000 simulations is output below:

```{r}
#| label: prep-s0.makeLmer-sim1000
#| eval: false

n_sims <- 1000

# test whether simulation works in principle
doTest(s0.makeLmer, fixed("polOri.dem-rep", "z"))

# do the simulation
out.s0.makeLmer <- powerSim(
  fit = s0.makeLmer,
  test = fixed("polOri.dem-rep", "z"),
  nsim = n_sims,
  progress = FALSE
)

# save simulation results in a data frame
time <- format(Sys.time(), "%Y%m%d_%H%M")
fileName <- paste0("out.s0.makeLmer", "_", time, ".RData")

save(
  out.s0.makeLmer,
  file = file.path("../data/powerAnalyses/simr", fileName)
)
```

```{r}
#| label: out-s0.makeLmer-sim1000

# load stored simulations (change file name if necessary)
fileName <- "out.s0.makeLmer_20240304_0924.RData"
load(file.path("../data/powerAnalyses/simr", fileName))

# put out results
out.s0.makeLmer
```

### Power Curves

We will calculate power curves for different changes in our default parameter values. The following list provides an overview regarding which changes we consider and how the corresponding R objects will be named.

-   **powC.default:** power curve for default parameters.

-   **powC.effectsize:** power curves for different effect sizes (10%, 20%, 30%, 40%, and 50% smaller than default effect size. The default effect size is based on the "empirical" simulations (see @fig-multiSim-deltaDuration-fixedEff).

-   **powC.effectsize.analytical:** power curves for two effect sizes that are derived from analytical considerations.

-   **powC.error:** power curves for different assumptions regarding noise (error / residual variance) in the data.

-   **powC.randomSlope:** power curves for different assumptions regarding random slope variance of political orientation.

-   **powC.items:** power curves for different numbers of items (trials).

#### Default Parameters

Lets inspect a power curve for this basic model for our default parameters in @fig-p.powC.default.

```{r}
#| label: prep-fig-p.powC.default
#| eval: false

# extend s0.makeLmer to include the max number of subjects
m.powC.default <- extend(
  s0.makeLmer,
  along = "subj",
  n = 1000
)

# create a power curve data set
powC.default <- powerCurve(
  fit = m.powC.default,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = seq(80, 300, 20),
  nsim = 1000,
  progress = FALSE
)
# plot(powC.default, power = .95)

# save simulation results in a data frame
time <- format(Sys.time(), "%Y%m%d_%H%M")
fileName <- paste0("powC.default", "_", time, ".RData")

save(
  powC.default,
  file = file.path("../data/powerAnalyses/simr", fileName)
)
```

```{r}
#| label: fig-p.powC.default
#| fig-cap: |
#|   Power curve for default parameters.

# load stored simulations (change file name if necessary)
fileName <- "powC.default_20240304_1348.RData"
load(file.path("../data/powerAnalyses/simr", fileName))

p.powC.default <- summary(powC.default) %>% 
  ggplot(aes(
    x = nlevels,
    y = mean,
    ymin = lower,
    ymax = upper
  )) +
  geom_ribbon(fill = "grey94") +
  geom_errorbar(color = "grey40", width = 1.5) +
  geom_line(color = "black") +
  geom_point() +
  geom_hline(yintercept = .95, color = "grey70", linetype = "dashed") +
  scale_x_continuous(breaks = summary(powC.default)$nlevels) +
  scale_y_continuous(
    limits = c(0, 1),
    breaks = seq(0, 1, .05),
    labels = paste0(seq(0, 100, 5), "%")
  ) +
  labs(
    x = "Number of Participants",
    y = "Power"
  ) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(color = "black")
  )

p.powC.default
```

#### Change Effect Size

##### "Empiric" Simulations

How does the relationship between subject sample size and power look like for different effect sizes? @fig-p.powC.effectsizes shows results from simulated power analyses for the default effect size (`r means_fixed$estimate.mean[2]`) and effect sizes that are 10%, 20%, 30%, 40%, and 50% smaller than this default effect size.

```{r}
#| label: prep-powC-effectSizes-parameters
#| results: false

# what is the estimated effect size in our simulated data?
fixef(s0.makeLmer)["polOri.dem-rep"]

# extend s0.makeLmer to include the max number of subjects
m.powC.effectsize <- extend(
  s0.makeLmer,
  along = "subj",
  n = 1000
)

# we will choose a smaller effect size and save this in a new model

# will choose the new effect size to be 10% smaller than the default
s1.makeLmer_minus10prcnt <- m.powC.effectsize
fixef(s1.makeLmer_minus10prcnt)["polOri.dem-rep"] <- (1-.1)*fixef(s0.makeLmer)["polOri.dem-rep"]

# will choose the new effect size to be 20% smaller than the default
s1.makeLmer_minus20prcnt <- m.powC.effectsize
fixef(s1.makeLmer_minus20prcnt)["polOri.dem-rep"] <- (1-.2)*fixef(s0.makeLmer)["polOri.dem-rep"]

# will choose the new effect size to be 30% smaller than the default
s1.makeLmer_minus30prcnt <- m.powC.effectsize
fixef(s1.makeLmer_minus30prcnt)["polOri.dem-rep"] <- (1-.3)*fixef(s0.makeLmer)["polOri.dem-rep"]

# will choose the new effect size to be 40% smaller than the default
s1.makeLmer_minus40prcnt <- m.powC.effectsize
fixef(s1.makeLmer_minus40prcnt)["polOri.dem-rep"] <- (1-.4)*fixef(s0.makeLmer)["polOri.dem-rep"]

# will choose the new effect size to be 50% smaller than the default
s1.makeLmer_minus50prcnt <- m.powC.effectsize
fixef(s1.makeLmer_minus50prcnt)["polOri.dem-rep"] <- (1-.5)*fixef(s0.makeLmer)["polOri.dem-rep"]

# test whether simulation will work
doTest(s1.makeLmer_minus50prcnt, fixed("polOri.dem-rep", "z"))
```

```{r}
#| label: prep-powC-effectSizes-doSims
#| eval: false

# how many simulations should be done for each number of subjects?
n_sims <- 10

# what are the breaks for number of subjects we would like to calculate power for?
breaks_subj <- seq(80, 300, 20)
 
# create power curve data sets

powC.effectsize_minus10prcnt <- powerCurve(
  fit = s1.makeLmer_minus10prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.effectsize_minus20prcnt <- powerCurve(
  fit = s1.makeLmer_minus20prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.effectsize_minus30prcnt <- powerCurve(
  fit = s1.makeLmer_minus30prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.effectsize_minus40prcnt <- powerCurve(
  fit = s1.makeLmer_minus40prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.effectsize_minus50prcnt <- powerCurve(
  fit = s1.makeLmer_minus50prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)

# combine power curve data sets
powC.effectsize <- rbind(
  summary(powC.default) %>% 
    mutate(type = "default"),
  summary(powC.effectsize_minus10prcnt) %>% 
    mutate(type = "effectsize.minus10%"),
  summary(powC.effectsize_minus20prcnt) %>% 
    mutate(type = "effectsize.minus20%"),
  summary(powC.effectsize_minus30prcnt) %>% 
    mutate(type = "effectsize.minus30%"),
  summary(powC.effectsize_minus40prcnt) %>% 
    mutate(type = "effectsize.minus40%"),
  summary(powC.effectsize_minus50prcnt) %>% 
    mutate(type = "effectsize.minus50%")
)

# save simulation results in a data frame
time <- format(Sys.time(), "%Y%m%d_%H%M")
fileName <- paste0("powC.effectsize", "_", time, ".RData")

save(
  powC.effectsize_minus10prcnt,
  powC.effectsize_minus20prcnt,
  powC.effectsize_minus30prcnt,
  powC.effectsize_minus40prcnt,
  powC.effectsize_minus50prcnt,
  powC.effectsize,
  file = file.path("../data/powerAnalyses/simr", fileName)
)
```

```{r}
#| label: prep-p.powC-function

FUNPlotPower <- function(myData,
                         myBreaks = seq(80, 300, 20),
                         myLegendTitle = waiver(),
                         myLegendLabels = waiver()) {
  plot <- myData %>% 
    ggplot(aes(
      x = nlevels,
      y = mean,
      ymin = lower,
      ymax = upper,
      color = type,
      fill = type
    )) +
    geom_ribbon(alpha = .1, color = NA) +
    geom_errorbar(width = 1.5) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept = .95, color = "grey70", linetype = "dashed") +
    scale_x_continuous(breaks = myBreaks) +
    scale_y_continuous(
      limits = c(0, 1),
      breaks = seq(0, 1, .05),
      labels = paste0(seq(0, 100, 5), "%")
    ) +
    ggthemes::scale_color_colorblind(labels = myLegendLabels) +
    ggthemes::scale_fill_colorblind(labels = myLegendLabels) +
    labs(
      x = "Number of Participants",
      y = "Power",
      color = myLegendTitle,
      fill = myLegendTitle
    ) +
    theme_bw() +
    theme(
      legend.position = c(.875, .21),
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      axis.line = element_line(color = "black")
    )
  return(plot)
}
```

```{r}
#| label: fig-p.powC.effectsizes
#| fig-cap: |
#|   Power curves for different effect sizes.

# load stored simulations (change file name if necessary)
fileName <- "powC.effectsize_20240304_1605.RData"
load(file.path("../data/powerAnalyses/simr", fileName))

# prepare data for plot: add + 2 on x axis for every type in order to prevent
# overprinting of errorbars
dataForPlot <- rbind(
  filter(powC.effectsize, type == "default"),
  filter(powC.effectsize, type == "effectsize.minus10%") %>% 
    mutate(nlevels = nlevels + 1*2),
  filter(powC.effectsize, type == "effectsize.minus20%") %>% 
    mutate(nlevels = nlevels + 2*2),
  filter(powC.effectsize, type == "effectsize.minus30%") %>% 
    mutate(nlevels = nlevels + 3*2),
  filter(powC.effectsize, type == "effectsize.minus40%") %>% 
    mutate(nlevels = nlevels + 4*2),
  filter(powC.effectsize, type == "effectsize.minus50%") %>% 
    mutate(nlevels = nlevels + 5*2)
)

tmp.legendLabels <- c(
  paste0("default = ", means_fixed$estimate.mean[2]),
  paste0("-", seq(10, 50, 10), "% = ", round(means_fixed$estimate.mean[2]*(1-seq(.1, .5, .1)), digits = 4))
)

# create and display plot
p.powC.effectsizes <- FUNPlotPower(
  myData= dataForPlot,
  myLegendTitle = "Effect Size",
  myLegendLabels = tmp.legendLabels
)
p.powC.effectsizes
```

##### Analytical Approach {#sec-analyticalApproach}

Based on our simulated data, our default model assumes an effect size of **`r means_fixed$estimate.mean[2]`**. That is, we expected that on average democrats would score `r means_fixed$estimate.mean[2]` higher on $\Delta Duration$ compared to republicans. This value is "empirically" simulated. In principle, the fixed effect that one should expect could also be calculated analytically:

$$
effect_{polOri} = \Delta Duration_{dem} - \Delta Duration_{rep}
$$

with

$$
\begin{split}
\Delta Duration_{dem} = \frac{(beta\_0\_c + beta\_p\_c \cdot X\_p ) - (beta\_0\_b + beta\_p\_b \cdot X\_p)}
                             {beta\_0\_c + beta\_p\_c \cdot X\_p + beta\_0\_b + beta\_p\_b \cdot X\_p} \\
                      = \frac{(3.5 + 0.5 \cdot 0.2) - (3.5 + 0.5 \cdot (-0.2))}
                             {3.5 + 0.5 \cdot 0.2 + 3.5 + 0.5 \cdot (-0.2)} \\
                      = \frac{0.2}{7} =
                        `r ((3.5+(.5)*.2)-(3.5-(.5)*0.2))/(3.5+(.5)*.2+3.5+(.5)*(-.2))`
\end{split}
$$

and

$$
\begin{split}
\Delta Duration_{rep} = \frac{(beta\_0\_c + beta\_p\_c \cdot X\_p ) - (beta\_0\_b + beta\_p\_b \cdot X\_p)}
                             {beta\_0\_c + beta\_p\_c \cdot X\_p + beta\_0\_b + beta\_p\_b \cdot X\_p} \\
                      = \frac{(3.5 + (-0.5) \cdot 0.2) - (3.5 + (-0.5) \cdot (-0.2))}
                             {3.5 + 0.2 + 3.5 + (-0.2)} \\
                      = \frac{-0.2}{7} =
                        `r ((3.5+(-.5)*.2)-(3.5-(-.5)*0.2))/(3.5+(-.5)*.2+3.5+(-.5)*(-.2))`
\end{split}
$$

Thus the "default" effect size according to the analytical approach is:

$$
\begin{split}
effect_{polOri} = \Delta Duration_{dem} - \Delta Duration_{rep} \\
  = \frac{0.2}{7} - \frac{-0.2}{7} = \frac{0.4}{7} \\
  = `r 0.4/7`
\end{split}
$$

Using this analytical approach, we can also calculate the expected effect size for the following scenario. Suppose that democrats display a higher dwell time on carbon information than republicans (as before), i.e., $beta\_p_\_c = 0.2$. But now suppose that democrats and republicans do not differ in their dwell time on bonus information, i.e. $beta\_p\_b = 0.0$. For this situation, we calculate:

$$
\begin{split}
\Delta Duration_{dem} = \frac{(3.5 + 0.5 \cdot 0.2) - (3.5 + 0.5 \cdot (0.0))}
                             {3.5 + 0.5 \cdot 0.2 + 3.5 + 0.5 \cdot (0.0)} \\
                      = \frac{0.1}{7.1} =
                        `r ((3.5+(.5)*.2)-(3.5-(.5)*0.0))/(3.5+(.5)*.2+3.5+(.5)*(0.0))`
\end{split}
$$

and

$$
\begin{split}
\Delta Duration_{rep} = \frac{(3.5 + (-0.5) \cdot 0.2) - (3.5 + (-0.5) \cdot (0.0))}
                             {3.5 + (-0.5) \cdot 0.2 + 3.5 + (-0.5) \cdot (0.0)} \\
                      = \frac{-0.1}{6.9} =
                        `r ((3.5+(-.5)*.2)-(3.5-(-.5)*0.0))/(3.5+(-.5)*.2+3.5+(-.5)*(0.0))`
\end{split}
$$

Thus the "smaller" effect size according to the analytical approach is:

$$
\begin{split}
effect_{polOri} = \Delta Duration_{dem} - \Delta Duration_{rep} \\
  = \frac{0.1}{7.1} - \frac{-0.1}{6.9} = `r 0.1/7.1--0.1/6.9`
\end{split}
$$

This analytical approach results in the power curve displayed in @fig-p.powC.effectsizes.analytical.

```{r}
#| label: prep-powC-effectSizes-analytical-parameters
#| results: false

# define analytical effect sizes
effSize.analytical_default <-  0.2/7 - -0.2/7
effSize.analytical_smaller <- 0.1/7.1 - -0.1/6.9

# extend s0.makeLmer to include the max number of subjects
m.powC.effectsize.analytical <- extend(
  s0.makeLmer,
  along = "subj",
  n = 1000
)

# we will choose a smaller effect size and save this in a new model

# will choose the new effect size to be the analytical default
s1.makeLmer.analytical_default <- m.powC.effectsize.analytical
fixef(s1.makeLmer.analytical_default)["polOri.dem-rep"] <- effSize.analytical_default

# will choose the new effect size to be analytical smaller effect size
s1.makeLmer.analytical_smaller <- m.powC.effectsize.analytical
fixef(s1.makeLmer.analytical_smaller)["polOri.dem-rep"] <- effSize.analytical_smaller

# test whether simulation will work
doTest(s1.makeLmer.analytical_smaller, fixed("polOri.dem-rep", "z"))
```

```{r}
#| label: prep-powC-effectSizes-analytical-doSims
#| eval: false

# how many simulations should be done for each number of subjects?
n_sims <- 10

# what are the breaks for number of subjects we would like to calculate power for?
breaks_subj <- seq(80, 300, 20)
 
# create power curve data sets

powC.effectsize.analytical_default <- powerCurve(
  fit = s1.makeLmer.analytical_default,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.effectsize.analytical_smaller <- powerCurve(
  fit = s1.makeLmer.analytical_smaller,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)

# combine power curve data sets
powC.effectsize.analytical <- rbind(
  summary(powC.effectsize.analytical_default) %>% 
    mutate(type = "default"),
  summary(powC.effectsize.analytical_smaller) %>% 
    mutate(type = "smaller")
)

# save simulation results in a data frame
time <- format(Sys.time(), "%Y%m%d_%H%M")
fileName <- paste0("powC.effectsize.analytical", "_", time, ".RData")

save(
  powC.effectsize.analytical_default,
  powC.effectsize.analytical_smaller,
  powC.effectsize.analytical,
  file = file.path("../data/powerAnalyses/simr", fileName)
)
```

```{r}
#| label: fig-p.powC.effectsizes.analytical
#| fig-cap: |
#|   Power curves for different effect sizes based on the analytical approach.

# load stored simulations (change file name if necessary)
fileName <- "powC.effectsize.analytical_20240304_1723.RData"
load(file.path("../data/powerAnalyses/simr", fileName))

# prepare data for plot: add + 2 on x axis for every type in order to prevent
# overprinting of errorbars
dataForPlot <- rbind(
  filter(powC.effectsize.analytical, type == "default"),
  filter(powC.effectsize.analytical, type == "smaller") %>% 
    mutate(nlevels = nlevels + 1*2)
)

tmp.legendLabels <- c(
  paste0("default = ", round(effSize.analytical_default, digits = 4)),
  paste0("smaller = ", round(effSize.analytical_smaller, digits = 4))
)

# create and display plot
p.powC.effectsizes.analytical <- FUNPlotPower(
  myData= dataForPlot,
  myLegendTitle = "Effect Size",
  myLegendLabels = tmp.legendLabels
)
p.powC.effectsizes.analytical
```

#### Change Residual Error

Power is usually heavily affected by the noise (i.e., error / residual variance) we assume to find in our data. Thus, we investigate how different assumptions regarding residual error affect the power to detect a fixed effect of political orientation on $\Delta Duration$. @fig-p.powC.error displays power curves for the assumed default error (`r means_random$estimate.mean[1]`) and errors that are 20%, 40%, 60%, 80%, 100%, 200%, and 300% higher than this default.

```{r}
#| label: prep-powC-error-parameters
#| results: false

# what is the estimated effect size in our simulated data?
sigma(s0.makeLmer)

# extend s0.makeLmer to include the max number of subjects
m.powC.error <- extend(
  s0.makeLmer,
  along = "subj",
  n = 1000
)

# we will choose greater residual variance and save this in a new model

# will choose the new residual variance to be 20% greater than the default
s1.makeLmer_plus20prcnt <- m.powC.error
sigma(s1.makeLmer_plus20prcnt) <- (1+.2)*sigma(s0.makeLmer)

# will choose the new residual variance to be 40% greater than the default
s1.makeLmer_plus40prcnt <- m.powC.error
sigma(s1.makeLmer_plus40prcnt) <- (1+.4)*sigma(s0.makeLmer)

# will choose the new residual variance to be 60% greater than the default
s1.makeLmer_plus60prcnt <- m.powC.error
sigma(s1.makeLmer_plus60prcnt) <- (1+.6)*sigma(s0.makeLmer)

# will choose the new residual variance to be 80% greater than the default
s1.makeLmer_plus80prcnt <- m.powC.error
sigma(s1.makeLmer_plus80prcnt) <- (1+.8)*sigma(s0.makeLmer)

# will choose the new residual variance to be 100% greater than the default
s1.makeLmer_plus100prcnt <- m.powC.error
sigma(s1.makeLmer_plus100prcnt) <- (1+1)*sigma(s0.makeLmer)

# will choose the new residual variance to be 200% greater than the default
s1.makeLmer_plus200prcnt <- m.powC.error
sigma(s1.makeLmer_plus200prcnt) <- (1+2)*sigma(s0.makeLmer)

# will choose the new residual variance to be 300% greater than the default
s1.makeLmer_plus300prcnt <- m.powC.error
sigma(s1.makeLmer_plus300prcnt) <- (1+3)*sigma(s0.makeLmer)

# test whether simulation will work
doTest(s1.makeLmer_plus300prcnt, fixed("polOri.dem-rep", "z"))
```

```{r}
#| label: prep-powC-error-doSims
#| eval: false

# how many simulations should be done for each number of subjects?
n_sims <- 10

# what are the breaks for number of subjects we would like to calculate power for?
breaks_subj <- seq(80, 300, 20)
 
# create power curve data sets

powC.error_plus20prcnt <- powerCurve(
  fit = s1.makeLmer_plus20prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.error_plus40prcnt <- powerCurve(
  fit = s1.makeLmer_plus40prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.error_plus60prcnt <- powerCurve(
  fit = s1.makeLmer_plus60prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.error_plus80prcnt <- powerCurve(
  fit = s1.makeLmer_plus80prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.error_plus100prcnt <- powerCurve(
  fit = s1.makeLmer_plus100prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.error_plus200prcnt <- powerCurve(
  fit = s1.makeLmer_plus200prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.error_plus300prcnt <- powerCurve(
  fit = s1.makeLmer_plus300prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)

# combine power curve data sets
powC.error <- rbind(
  summary(powC.default) %>% 
    mutate(type = "default"),
  summary(powC.error_plus20prcnt) %>% 
    mutate(type = "error.plus20%"),
  summary(powC.error_plus40prcnt) %>% 
    mutate(type = "error.plus40%"),
  summary(powC.error_plus60prcnt) %>% 
    mutate(type = "error.plus60%"),
  summary(powC.error_plus80prcnt) %>% 
    mutate(type = "error.plus80%"),
  summary(powC.error_plus100prcnt) %>% 
    mutate(type = "error.plus100%"),
  summary(powC.error_plus200prcnt) %>% 
    mutate(type = "error.plus200%"),
  summary(powC.error_plus300prcnt) %>% 
    mutate(type = "error.plus300%")
)

# save simulation results in a data frame
time <- format(Sys.time(), "%Y%m%d_%H%M")
fileName <- paste0("powC.error", "_", time, ".RData")

save(
  powC.error_plus20prcnt,
  powC.error_plus40prcnt,
  powC.error_plus60prcnt,
  powC.error_plus80prcnt,
  powC.error_plus100prcnt,
  powC.error_plus200prcnt,
  powC.error_plus300prcnt,
  powC.error,
  file = file.path("../data/powerAnalyses/simr", fileName)
)
```

```{r}
#| label: fig-p.powC.error
#| fig-cap: |
#|   Power curves for different residual variances.

# load stored simulations (change file name if necessary)
fileName <- "powC.error_20240305_1052.RData"
load(file.path("../data/powerAnalyses/simr", fileName))

# prepare data for plot: add + 2 on x axis for every type in order to prevent
# overprinting of errorbars
dataForPlot <- rbind(
  filter(powC.error, type == "default"),
  filter(powC.error, type == "error.plus20%") %>% 
    mutate(nlevels = nlevels + 1*2),
  filter(powC.error, type == "error.plus40%") %>% 
    mutate(nlevels = nlevels + 2*2),
  filter(powC.error, type == "error.plus60%") %>% 
    mutate(nlevels = nlevels + 3*2),
  filter(powC.error, type == "error.plus80%") %>% 
    mutate(nlevels = nlevels + 4*2),
  filter(powC.error, type == "error.plus100%") %>% 
    mutate(nlevels = nlevels + 5*2),
  filter(powC.error, type == "error.plus200%") %>% 
    mutate(nlevels = nlevels + 6*2),
  filter(powC.error, type == "error.plus300%") %>% 
    mutate(nlevels = nlevels + 7*2)
) %>% 
  mutate(type = factor(type, levels = c(
    "default",
    "error.plus20%",
    "error.plus40%",
    "error.plus60%",
    "error.plus80%",
    "error.plus100%",
    "error.plus200%",
    "error.plus300%"
  )))

tmp.legendLabels <- c(
  paste0("default = ", means_random$estimate.mean[1]),
  paste0("+", c(seq(20, 100, 20), 200, 300), "% = ", round(means_random$estimate.mean[1]*(1+c(seq(.2, 1, .2), 2, 3)), digits = 4))
)

# create and display plot
p.powC.error <- FUNPlotPower(
  myData= dataForPlot,
  myLegendTitle = "Residual Error (SD)",
  myLegendLabels = tmp.legendLabels
)
p.powC.error + 
  theme(legend.position = c(.875, .28))
```

#### Change Random Slope

The power to detect a true effect of political orientation can be affected by how much subjects vary in this effect, that is how big the random slope variance of political orientation is. @fig-p.powC.randomSlope shows that while greater random slope variance indeed reduces statistical power, one can still achieve good power for reasonable sample sizes even if the expected random slope variance is increased by 500% or 1000%.

```{r}
#| label: prep-powC-randomSlope-parameters
#| results: false

# what is the estimated random slope in our simulated data?
VarCorr(s0.makeLmer) %>% 
  as_tibble() %>% 
  filter(var1 == "polOri.dem-rep")

# to change the random slope, we have to change the variance-covariance matrix
# we defined this matrix above. lets do this again for clarity:
# 
# V_new <- matrix(c(
#   e1, 0,
#   0, e4), nrow = 2)
# VarCorr(s0.makeLmer) <- list(
#   V_new,
#   e5
# )
# 
# we will need to change e4 (random slope variance)
# we repeat the definition of e1, e4, and e5 for clarity
e1 <- VarCorr(s0.makeLmer) %>%
  as_tibble() %>% 
  filter(grp == "subj", var1 == "(Intercept)", is.na(var2)) %>% 
  pull(vcov)
e4 <- VarCorr(s0.makeLmer) %>%
  as_tibble() %>% 
  filter(grp == "subj", var1 == "polOri.dem-rep", is.na(var2)) %>% 
  pull(vcov)
e5 <- VarCorr(s0.makeLmer) %>%
  as_tibble() %>% 
  filter(grp == "item", var1 == "(Intercept)", is.na(var2)) %>% 
  pull(vcov)

# extend s0.makeLmer to include the max number of subjects
m.powC.randomSlope <- extend(
  s0.makeLmer,
  along = "subj",
  n = 1000
)

# we will choose a higher random slope and save this in a new model

# we will choose the new random slope variance to be 200% greater than the default
s3.makeLmer_plus500prcnt <- m.powC.randomSlope
VarCorr(s3.makeLmer_plus500prcnt) <- list(
  matrix(c(e1, 0, 0, (1 + 5)*e4), 2),
  e5
)

# we will choose the new random slope variance to be 200% greater than the default
s3.makeLmer_plus1000prcnt <- m.powC.randomSlope
VarCorr(s3.makeLmer_plus1000prcnt) <- list(
  matrix(c(e1, 0, 0, (1 + 10)*e4), 2),
  e5
)

# test whether manipulation worked
VarCorr(s0.makeLmer) %>% 
  as_tibble()
VarCorr(s3.makeLmer_plus1000prcnt) %>% 
  as_tibble()

# test whether simulation will work
doTest(s3.makeLmer_plus1000prcnt, fixed("polOri.dem-rep", "z"))
```

```{r}
#| label: prep-powC-randomSlope-doSims
#| eval: false

# how many simulations should be done for each number of subjects?
n_sims <- 1000

# what are the breaks for number of subjects we would like to calculate power for?
breaks_subj <- seq(80, 300, 20)
 
# create power curve data sets

powC.randomSlope_plus500prcnt <- powerCurve(
  fit = s3.makeLmer_plus500prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.randomSlope_plus1000prcnt <- powerCurve(
  fit = s3.makeLmer_plus1000prcnt,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)

# combine power curve data sets
powC.randomSlope <- rbind(
  summary(powC.default) %>% 
    mutate(type = "default"),
  summary(powC.randomSlope_plus500prcnt) %>% 
    mutate(type = "randomSlope.plus500%"),
  summary(powC.randomSlope_plus1000prcnt) %>% 
    mutate(type = "randomSlope.plus1000%")
)

# save simulation results in a data frame
time <- format(Sys.time(), "%Y%m%d_%H%M")
fileName <- paste0("powC.randomSlope", "_", time, ".RData")

save(
  powC.randomSlope_plus500prcnt,
  powC.randomSlope_plus1000prcnt,
  powC.randomSlope,
  file = file.path("../data/powerAnalyses/simr", fileName)
)
```

```{r}
#| label: fig-p.powC.randomSlope
#| fig-cap: |
#|   Power curves for different random slope variances.

# load stored simulations (change file name if necessary)
fileName <- "powC.randomSlope_20240305_1157.RData"
load(file.path("../data/powerAnalyses/simr", fileName))

# prepare data for plot: add + 2 on x axis for every type in order to prevent
# overprinting of errorbars
dataForPlot <- rbind(
  filter(powC.randomSlope, type == "default"),
  filter(powC.randomSlope, type == "randomSlope.plus500%") %>% 
    mutate(nlevels = nlevels + 1*2),
  filter(powC.randomSlope, type == "randomSlope.plus1000%") %>% 
    mutate(nlevels = nlevels + 2*2)
) %>% 
  mutate(type = factor(type, levels = c(
    "default",
    "randomSlope.plus500%",
    "randomSlope.plus1000%"
  )))

tmp.legendLabels <- c(
  paste0("default = ", round(means_random$estimate.mean[5]^2, digits = 6)),
  paste0("+", seq(500, 1000, 500), "% = ", round(means_random$estimate.mean[5]^2*(1+seq(5, 10, 5)), digits = 6))
)

# create and display plot
p.powC.randomSlope <- FUNPlotPower(
  myData= dataForPlot,
  myLegendTitle = "Random Slope (Var)",
  myLegendLabels = tmp.legendLabels
)
p.powC.randomSlope
```

#### Change Number of Items

To increase statistical power, we can not only test more subjects but we can also increase the number of items (trials) each subject answers. In @fig-p.powC.effectsizes.analytical, we saw that the smaller effect size (assuming that democrats and republicans only differ in dwell time on carbon but not on bonus information) struggled with low power. In @fig-p.powC.items, we report power curves for this smaller analytical effect size for different numbers of items (25, 36, 49, and 64). Note that the X-axis (Number of Participants) ranges from 100 to 400 (not from 80 to 300 as in all plots before).

```{r}
#| label: prep-powC-items-parameters
#| results: false

# extend s0.makeLmer to include the max number of subjects
m.powC.items <- extend(
  s0.makeLmer,
  along = "subj",
  n = 1000
)
# extend s0.makeLmer to include 36 items
m.powC.items_25 <- extend(
  m.powC.items,
  along = "item",
  n = 25
)
# extend s0.makeLmer to include 36 items
m.powC.items_36 <- extend(
  m.powC.items,
  along = "item",
  n = 36
)
# extend s0.makeLmer to include 49 items
m.powC.items_49 <- extend(
  m.powC.items,
  along = "item",
  n = 49
)
# extend s0.makeLmer to include 64 items
m.powC.items_64 <- extend(
  m.powC.items,
  along = "item",
  n = 64
)

# we will choose the smaller analytical effect as effect size with 36 items
s4.makeLmer.25_smaller <- m.powC.items_25
fixef(s4.makeLmer.25_smaller)["polOri.dem-rep"] <- effSize.analytical_smaller

# we will choose the smaller analytical effect as effect size with 36 items
s4.makeLmer.36_smaller <- m.powC.items_36
fixef(s4.makeLmer.36_smaller)["polOri.dem-rep"] <- effSize.analytical_smaller

# we will choose the smaller analytical effect as effect size with 49 items
s4.makeLmer.49_smaller <- m.powC.items_49
fixef(s4.makeLmer.49_smaller)["polOri.dem-rep"] <- effSize.analytical_smaller

# we will choose the smaller analytical effect as effect size with 49 items
s4.makeLmer.64_smaller <- m.powC.items_64
fixef(s4.makeLmer.64_smaller)["polOri.dem-rep"] <- effSize.analytical_smaller

# test whether simulation will work
doTest(s1.makeLmer.analytical_smaller, fixed("polOri.dem-rep", "z"))
doTest(s4.makeLmer.64_smaller, fixed("polOri.dem-rep", "z"))
```

```{r}
#| label: prep-powC-items-doSims
#| eval: false

# how many simulations should be done for each number of subjects?
n_sims <- 10

# what are the breaks for number of subjects we would like to calculate power for?
breaks_subj <- seq(100, 400, 50)
 
# create power curve data sets

powC.items_25_smaller <- powerCurve(
  fit = s4.makeLmer.25_smaller,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.items_36_smaller <- powerCurve(
  fit = s4.makeLmer.36_smaller,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.items_49_smaller <- powerCurve(
  fit = s4.makeLmer.49_smaller,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)
powC.items_64_smaller <- powerCurve(
  fit = s4.makeLmer.64_smaller,
  test = fixed("polOri.dem-rep", method = "z"),
  along = "subj",
  breaks = breaks_subj,
  nsim = n_sims,
  progress = FALSE
)

# combine power curve data sets
powC.items <- rbind(
  summary(powC.items_25_smaller) %>% 
    mutate(type = "items.25"),
  summary(powC.items_36_smaller) %>% 
    mutate(type = "items.36"),
  summary(powC.items_49_smaller) %>% 
    mutate(type = "items.49"),
  summary(powC.items_64_smaller) %>% 
    mutate(type = "items.64")
)

# save simulation results in a data frame
time <- format(Sys.time(), "%Y%m%d_%H%M")
fileName <- paste0("powC.items", "_", time, ".RData")

save(
  powC.items_25_smaller,
  powC.items_36_smaller,
  powC.items_49_smaller,
  powC.items_64_smaller,
  powC.items,
  file = file.path("../data/powerAnalyses/simr", fileName)
)
```

```{r}
#| label: fig-p.powC.items
#| fig-cap: |
#|   Power curves for different numbers of items.

# load stored simulations (change file name if necessary)
fileName <- "powC.items_20240305_1456.RData"
load(file.path("../data/powerAnalyses/simr", fileName))

# prepare data for plot: add + 2 on x axis for every type in order to prevent
# overprinting of errorbars
dataForPlot <- rbind(
  filter(powC.items, type == "items.25"),
  filter(powC.items, type == "items.36") %>% 
    mutate(nlevels = nlevels + 1*2),
  filter(powC.items, type == "items.49") %>% 
    mutate(nlevels = nlevels + 2*2),
  filter(powC.items, type == "items.64") %>% 
    mutate(nlevels = nlevels + 3*2)
) %>% 
  mutate(type = factor(type, levels = c(
    "items.25",
    "items.36",
    "items.49",
    "items.64"
  )))

tmp.legendLabels <- seq(5, 8, 1)^2

# create and display plot
p.powC.items <- FUNPlotPower(
  myData= dataForPlot,
  myLegendTitle = "Number of Items",
  myLegendLabels = tmp.legendLabels,
  myBreaks = seq(100, 400, 50)
)
p.powC.items
```

## Custom Simulations

Besides doing power analyses using `simr`, we could also do power analyses in customized functions we define ourselves. For instance, `simr` cannot make use of `brms::brm` to fit data (which we will likely use in the end because of its superior computational power and efficiency). Similarly, it is easier to simulate unbalanced designs in a custom function. Finally, custom power analysis functions would also allow to simulate true correlations between random intercepts and random slopes (which we set to zero up to now). For now, however, the power analyses reported above using `simr` should do the job.
